{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e92888f",
   "metadata": {},
   "source": [
    "## 1. Préambule\n",
    "\n",
    "### 1.1 Problématique\n",
    "\n",
    "**Fruits!** startup wants to develop a mobile application for fruit recognition. The goal is to create a scalable image classification pipeline that can:\n",
    "\n",
    "- Process large volumes of fruit images (90,000+ images)\n",
    "- Extract meaningful features using transfer learning (MobileNetV2)\n",
    "- Reduce dimensionality for efficient storage and processing\n",
    "- Scale horizontally in a Big Data cloud environment\n",
    "\n",
    "### 1.2 Objectifs\n",
    "\n",
    "1. **Feature Extraction**: Use MobileNetV2 pretrained on ImageNet to extract 1280-dimensional features\n",
    "2. **Dimensionality Reduction**: Apply PCA to reduce features while preserving variance\n",
    "3. **Distributed Processing**: Leverage PySpark for scalable, distributed computation\n",
    "4. **Cloud-Ready**: Design pipeline compatible with AWS EMR / Databricks\n",
    "\n",
    "### 1.3 Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    Local Development (Docker)                    │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  ┌──────────┐    ┌──────────────┐    ┌─────────────────┐       │\n",
    "│  │  Images  │───▶│  PySpark     │───▶│  PyTorch GPU    │       │\n",
    "│  │  (S3/Local)   │  DataFrames  │    │  MobileNetV2    │       │\n",
    "│  └──────────┘    └──────────────┘    └────────┬────────┘       │\n",
    "│                                               │                 │\n",
    "│                                               ▼                 │\n",
    "│  ┌──────────┐    ┌──────────────┐    ┌─────────────────┐       │\n",
    "│  │  Output  │◀───│  PCA         │◀───│  Features       │       │\n",
    "│  │  (Parquet)    │  Reduction   │    │  (1280-dim)     │       │\n",
    "│  └──────────┘    └──────────────┘    └─────────────────┘       │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  MLflow Tracking: http://localhost:5009                         │\n",
    "│  Spark UI: http://localhost:4049                                │\n",
    "│  Jupyter: http://localhost:8889                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5762a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Environment Setup\n",
    "\n",
    "### 2.1 Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2132f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '/app/src')\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ea017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'DATASET_PATH': '/app/dataset/fruits-360_dataset/fruits-360',\n",
    "    'OUTPUT_PATH': '/app/data/Results',\n",
    "    \n",
    "    # Processing\n",
    "    'BATCH_SIZE': 32,\n",
    "    'PCA_COMPONENTS': 50,\n",
    "    \n",
    "    # Spark\n",
    "    'SPARK_DRIVER_MEMORY': '4g',\n",
    "    'SPARK_EXECUTOR_MEMORY': '4g',\n",
    "    \n",
    "    # MLflow\n",
    "    'MLFLOW_TRACKING_URI': 'http://mlflow:5000',\n",
    "    'MLFLOW_EXPERIMENT': 'mission9_experiments',\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['OUTPUT_PATH'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b7b68e",
   "metadata": {},
   "source": [
    "### 2.2 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113777b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Mission9_Fruits360') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory', CONFIG['SPARK_DRIVER_MEMORY']) \\\n",
    "    .config('spark.executor.memory', CONFIG['SPARK_EXECUTOR_MEMORY']) \\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', 'true') \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', '1000') \\\n",
    "    .config('spark.sql.parquet.writeLegacyFormat', 'true') \\\n",
    "    .config('spark.executor.resource.gpu.amount', '1') \\\n",
    "    .config('spark.task.resource.gpu.amount', '0.25') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: http://localhost:4049\")\n",
    "print(f\"Default parallelism: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5a48b",
   "metadata": {},
   "source": [
    "### 2.3 MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e2fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLflow\n",
    "mlflow.set_tracking_uri(CONFIG['MLFLOW_TRACKING_URI'])\n",
    "mlflow.set_experiment(CONFIG['MLFLOW_EXPERIMENT'])\n",
    "\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow UI: http://localhost:5009\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512aec2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Loading\n",
    "\n",
    "### 3.1 Dataset Overview\n",
    "\n",
    "The **Fruits-360** dataset contains:\n",
    "- **90,483 images** of fruits and vegetables\n",
    "- **131 classes** (apple varieties, bananas, oranges, etc.)\n",
    "- **100x100 pixel** RGB images\n",
    "- **Training set**: 67,692 images\n",
    "- **Test set**: 22,688 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ad0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset structure\n",
    "dataset_path = Path(CONFIG['DATASET_PATH'])\n",
    "\n",
    "if dataset_path.exists():\n",
    "    print(f\"Dataset found at: {dataset_path}\")\n",
    "    \n",
    "    for split in ['Training', 'Test']:\n",
    "        split_path = dataset_path / split\n",
    "        if split_path.exists():\n",
    "            classes = list(split_path.iterdir())\n",
    "            n_images = sum(len(list((split_path / c.name).glob('*.jpg'))) for c in classes if c.is_dir())\n",
    "            print(f\"  {split}: {len(classes)} classes, {n_images:,} images\")\n",
    "else:\n",
    "    print(f\"Dataset not found at: {dataset_path}\")\n",
    "    print(\"\\nTo download the dataset:\")\n",
    "    print(\"1. Visit: https://www.kaggle.com/datasets/moltean/fruits\")\n",
    "    print(\"2. Download and extract to: dataset/fruits-360_dataset/\")\n",
    "    print(\"\\nOr generate a subset for testing:\")\n",
    "    print(\"  python scripts/subset_data.py --percentage 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64094b47",
   "metadata": {},
   "source": [
    "### 3.2 Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path: str, split: str = 'Training') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Load images from a directory using Spark's binaryFile reader.\n",
    "    \n",
    "    Args:\n",
    "        path: Base path to dataset\n",
    "        split: 'Training' or 'Test'\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame with columns: path, content, label, filename\n",
    "    \"\"\"\n",
    "    full_path = f\"{path}/{split}\"\n",
    "    \n",
    "    # Load binary files\n",
    "    df = spark.read.format('binaryFile') \\\n",
    "        .option('pathGlobFilter', '*.jpg') \\\n",
    "        .option('recursiveFileLookup', 'true') \\\n",
    "        .load(full_path)\n",
    "    \n",
    "    # Extract label from path\n",
    "    df = df.withColumn(\n",
    "        'label',\n",
    "        F.element_at(F.split(F.col('path'), '/'), -2)\n",
    "    )\n",
    "    \n",
    "    # Extract filename\n",
    "    df = df.withColumn(\n",
    "        'filename',\n",
    "        F.element_at(F.split(F.col('path'), '/'), -1)\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4822570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"Loading training images...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_train = load_images(CONFIG['DATASET_PATH'], 'Training')\n",
    "train_count = df_train.count()\n",
    "\n",
    "print(f\"Loaded {train_count:,} training images in {time.time() - start_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test images...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_test = load_images(CONFIG['DATASET_PATH'], 'Test')\n",
    "test_count = df_test.count()\n",
    "\n",
    "print(f\"Loaded {test_count:,} test images in {time.time() - start_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4786f",
   "metadata": {},
   "source": [
    "### 3.3 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "print(\"Sample training data:\")\n",
    "df_train.select('path', 'label', 'filename', 'length').show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "print(\"Class distribution (top 20):\")\n",
    "df_train.groupBy('label') \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc('count')) \\\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique classes\n",
    "n_classes = df_train.select('label').distinct().count()\n",
    "print(f\"Total unique classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6eb897",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Extraction\n",
    "\n",
    "### 4.1 MobileNetV2 Setup\n",
    "\n",
    "We use **MobileNetV2** pretrained on ImageNet for transfer learning:\n",
    "- Efficient architecture designed for mobile devices\n",
    "- 1280-dimensional feature vectors from the penultimate layer\n",
    "- GPU-accelerated inference with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48cf8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature extractor\n",
    "from feature_extractor import MobileNetV2Extractor\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = MobileNetV2Extractor(\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    batch_size=CONFIG['BATCH_SIZE']\n",
    ")\n",
    "\n",
    "# Display model info\n",
    "print(\"\\nModel Information:\")\n",
    "for key, value in extractor.get_model_info().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature extraction on a sample image\n",
    "sample_row = df_train.select('content', 'label').first()\n",
    "sample_features = extractor.extract(bytes(sample_row['content']))\n",
    "\n",
    "print(f\"Sample label: {sample_row['label']}\")\n",
    "print(f\"Feature shape: {sample_features.shape}\")\n",
    "print(f\"Feature range: [{sample_features.min():.4f}, {sample_features.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddc0c7",
   "metadata": {},
   "source": [
    "### 4.2 Distributed Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "# Create Pandas UDF for distributed feature extraction\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def extract_features_udf(content_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Pandas UDF for batch feature extraction.\n",
    "    Uses GPU-accelerated MobileNetV2.\n",
    "    \"\"\"\n",
    "    # Initialize extractor on worker (cached)\n",
    "    if not hasattr(extract_features_udf, '_extractor'):\n",
    "        # Limit GPU memory usage for PyTorch\n",
    "        if torch.cuda.is_available():\n",
    "            # Set memory fraction to limit usage (e.g., 6GB out of total)\n",
    "            # Assuming ~24GB total, 0.25 is approx 6GB\n",
    "            torch.cuda.set_per_process_memory_fraction(0.25)\n",
    "            \n",
    "        extract_features_udf._extractor = MobileNetV2Extractor(\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "    \n",
    "    extractor = extract_features_udf._extractor\n",
    "    \n",
    "    # Process batch\n",
    "    images = [bytes(img) for img in content_series]\n",
    "    features = extractor.extract_batch(images)\n",
    "    \n",
    "    return pd.Series([f.tolist() for f in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11877025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 40624)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 766, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Start MLflow run\n",
    "run_name = f\"feature_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    # Log parameters\n",
    "    mlflow.log_param('dataset_path', CONFIG['DATASET_PATH'])\n",
    "    mlflow.log_param('batch_size', CONFIG['BATCH_SIZE'])\n",
    "    mlflow.log_param('pca_components', CONFIG['PCA_COMPONENTS'])\n",
    "    mlflow.log_param('train_count', train_count)\n",
    "    mlflow.log_param('test_count', test_count)\n",
    "    mlflow.log_param('n_classes', n_classes)\n",
    "    \n",
    "    # Log model info\n",
    "    for key, value in extractor.get_model_info().items():\n",
    "        mlflow.log_param(f'model_{key}', value)\n",
    "    \n",
    "    # Extract features from training set\n",
    "    print(\"\\nExtracting training features...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_train_features = df_train.withColumn(\n",
    "        'features',\n",
    "        extract_features_udf(F.col('content'))\n",
    "    )\n",
    "    \n",
    "    # Trigger computation and cache\n",
    "    df_train_features = df_train_features.cache()\n",
    "    _ = df_train_features.count()\n",
    "    \n",
    "    train_extraction_time = time.time() - start_time\n",
    "    print(f\"Training features extracted in {train_extraction_time:.1f}s\")\n",
    "    mlflow.log_metric('train_extraction_time', train_extraction_time)\n",
    "    \n",
    "    # Extract features from test set\n",
    "    print(\"\\nExtracting test features...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_test_features = df_test.withColumn(\n",
    "        'features',\n",
    "        extract_features_udf(F.col('content'))\n",
    "    )\n",
    "    \n",
    "    df_test_features = df_test_features.cache()\n",
    "    _ = df_test_features.count()\n",
    "    \n",
    "    test_extraction_time = time.time() - start_time\n",
    "    print(f\"Test features extracted in {test_extraction_time:.1f}s\")\n",
    "    mlflow.log_metric('test_extraction_time', test_extraction_time)\n",
    "    \n",
    "    print(f\"\\n✓ Feature extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6721c",
   "metadata": {},
   "source": [
    "### 4.3 Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfa28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify feature extraction\n",
    "sample = df_train_features.select('label', 'features').first()\n",
    "print(f\"Label: {sample['label']}\")\n",
    "print(f\"Feature dimension: {len(sample['features'])}\")\n",
    "print(f\"First 10 features: {sample['features'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c20da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. PCA Dimensionality Reduction\n",
    "\n",
    "### 5.1 Apply PCA\n",
    "\n",
    "We reduce the 1280-dimensional features to a lower dimension using PCA:\n",
    "- Reduces storage and computation costs\n",
    "- Removes noise and redundant features\n",
    "- Prepares data for downstream classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ee893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "\n",
    "# Convert features array to Vector type\n",
    "@F.udf(VectorUDT())\n",
    "def to_vector(features):\n",
    "    return Vectors.dense(features)\n",
    "\n",
    "# Apply to training data\n",
    "df_train_vec = df_train_features.withColumn(\n",
    "    'features_vector',\n",
    "    to_vector(F.col('features'))\n",
    ")\n",
    "\n",
    "# Apply to test data\n",
    "df_test_vec = df_test_features.withColumn(\n",
    "    'features_vector',\n",
    "    to_vector(F.col('features'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features before PCA\n",
    "scaler = StandardScaler(\n",
    "    inputCol='features_vector',\n",
    "    outputCol='features_scaled',\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler_model = scaler.fit(df_train_vec)\n",
    "\n",
    "# Transform both datasets\n",
    "df_train_scaled = scaler_model.transform(df_train_vec)\n",
    "df_test_scaled = scaler_model.transform(df_test_vec)\n",
    "\n",
    "print(\"Features standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2269c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "n_components = CONFIG['PCA_COMPONENTS']\n",
    "\n",
    "print(f\"Fitting PCA with {n_components} components...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pca = PCA(\n",
    "    k=n_components,\n",
    "    inputCol='features_scaled',\n",
    "    outputCol='pca_features'\n",
    ")\n",
    "\n",
    "# Fit PCA on training data\n",
    "pca_model = pca.fit(df_train_scaled)\n",
    "\n",
    "pca_fit_time = time.time() - start_time\n",
    "print(f\"PCA fitted in {pca_fit_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform datasets\n",
    "df_train_pca = pca_model.transform(df_train_scaled)\n",
    "df_test_pca = pca_model.transform(df_test_scaled)\n",
    "\n",
    "print(\"PCA transformation applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cef1cf",
   "metadata": {},
   "source": [
    "### 5.2 Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze explained variance\n",
    "explained_variance = pca_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"\\nPCA Explained Variance Analysis:\")\n",
    "print(f\"  Components: {n_components}\")\n",
    "print(f\"  Total explained variance: {cumulative_variance[-1]*100:.2f}%\")\n",
    "print(f\"\\nVariance by component (first 10):\")\n",
    "for i in range(min(10, len(explained_variance))):\n",
    "    print(f\"  PC{i+1}: {explained_variance[i]*100:.2f}% (cumulative: {cumulative_variance[i]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9695d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log PCA results to MLflow\n",
    "with mlflow.start_run(run_name=f\"pca_{n_components}comp\"):\n",
    "    mlflow.log_param('n_components', n_components)\n",
    "    mlflow.log_metric('explained_variance_total', cumulative_variance[-1])\n",
    "    mlflow.log_metric('pca_fit_time', pca_fit_time)\n",
    "    \n",
    "    # Log variance per component\n",
    "    for i, var in enumerate(explained_variance[:10]):\n",
    "        mlflow.log_metric(f'var_pc{i+1}', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f181c",
   "metadata": {},
   "source": [
    "### 5.3 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, len(explained_variance) + 1), explained_variance * 100)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance (%)')\n",
    "axes[0].set_title('Explained Variance per Component')\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, 'b-o')\n",
    "axes[1].axhline(y=95, color='r', linestyle='--', label='95% threshold')\n",
    "axes[1].axhline(y=99, color='g', linestyle='--', label='99% threshold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance (%)')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['OUTPUT_PATH']}/pca_variance.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to: {CONFIG['OUTPUT_PATH']}/pca_variance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed796c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Results Export\n",
    "\n",
    "### 6.1 Save to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0dd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "# Convert PCA vector to array for easier storage\n",
    "@F.udf(ArrayType(FloatType()))\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().tolist()\n",
    "\n",
    "# Prepare training output\n",
    "df_train_output = df_train_pca.select(\n",
    "    'path',\n",
    "    'label',\n",
    "    'filename',\n",
    "    vector_to_array('pca_features').alias('pca_features')\n",
    ")\n",
    "\n",
    "# Prepare test output\n",
    "df_test_output = df_test_pca.select(\n",
    "    'path',\n",
    "    'label',\n",
    "    'filename',\n",
    "    vector_to_array('pca_features').alias('pca_features')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training results to Parquet\n",
    "train_output_path = f\"{CONFIG['OUTPUT_PATH']}/training_pca\"\n",
    "print(f\"Saving training results to: {train_output_path}\")\n",
    "\n",
    "df_train_output.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('label') \\\n",
    "    .parquet(train_output_path)\n",
    "\n",
    "print(\"✓ Training results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results to Parquet\n",
    "test_output_path = f\"{CONFIG['OUTPUT_PATH']}/test_pca\"\n",
    "print(f\"Saving test results to: {test_output_path}\")\n",
    "\n",
    "df_test_output.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('label') \\\n",
    "    .parquet(test_output_path)\n",
    "\n",
    "print(\"✓ Test results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583e957",
   "metadata": {},
   "source": [
    "### 6.2 Export to CSV\n",
    "\n",
    "Export to CSV format for compatibility with other tools and cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba5fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(df, output_path: str, n_components: int):\n",
    "    \"\"\"\n",
    "    Export PCA results to CSV with individual feature columns.\n",
    "    \"\"\"\n",
    "    # Add individual feature columns\n",
    "    df_csv = df.select(\n",
    "        'label',\n",
    "        'filename',\n",
    "        'pca_features'\n",
    "    )\n",
    "    \n",
    "    # Explode array to columns\n",
    "    for i in range(n_components):\n",
    "        df_csv = df_csv.withColumn(f'f_{i}', F.col('pca_features')[i])\n",
    "    \n",
    "    # Select final columns\n",
    "    feature_cols = [f'f_{i}' for i in range(n_components)]\n",
    "    df_csv = df_csv.select('label', 'filename', *feature_cols)\n",
    "    \n",
    "    # Write to CSV\n",
    "    df_csv.coalesce(1).write \\\n",
    "        .mode('overwrite') \\\n",
    "        .option('header', 'true') \\\n",
    "        .csv(output_path)\n",
    "    \n",
    "    return df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affa07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training to CSV\n",
    "train_csv_path = f\"{CONFIG['OUTPUT_PATH']}/training_pca_csv\"\n",
    "print(f\"Exporting training CSV to: {train_csv_path}\")\n",
    "\n",
    "export_to_csv(df_train_output, train_csv_path, n_components)\n",
    "print(\"✓ Training CSV exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ed73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export test to CSV\n",
    "test_csv_path = f\"{CONFIG['OUTPUT_PATH']}/test_pca_csv\"\n",
    "print(f\"Exporting test CSV to: {test_csv_path}\")\n",
    "\n",
    "export_to_csv(df_test_output, test_csv_path, n_components)\n",
    "print(\"✓ Test CSV exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c22d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Scaling Analysis\n",
    "\n",
    "### 7.1 Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d220546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary metrics\n",
    "print(\"=\"*60)\n",
    "print(\"Pipeline Performance Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training images: {train_count:,}\")\n",
    "print(f\"  Test images: {test_count:,}\")\n",
    "print(f\"  Total classes: {n_classes}\")\n",
    "\n",
    "print(f\"\\nFeature Extraction:\")\n",
    "print(f\"  Model: MobileNetV2 (ImageNet)\")\n",
    "print(f\"  Original features: 1,280\")\n",
    "print(f\"  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "print(f\"\\nPCA Reduction:\")\n",
    "print(f\"  Components: {n_components}\")\n",
    "print(f\"  Explained variance: {cumulative_variance[-1]*100:.2f}%\")\n",
    "print(f\"  Compression ratio: {1280/n_components:.1f}x\")\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Training Parquet: {train_output_path}\")\n",
    "print(f\"  Test Parquet: {test_output_path}\")\n",
    "print(f\"  Training CSV: {train_csv_path}\")\n",
    "print(f\"  Test CSV: {test_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc94eee",
   "metadata": {},
   "source": [
    "### 7.2 Scalability Tests\n",
    "\n",
    "For scalability testing, use the subset generator to create progressively larger datasets:\n",
    "\n",
    "```bash\n",
    "# Generate subsets\n",
    "python scripts/subset_data.py --percentage 1   # ~900 images\n",
    "python scripts/subset_data.py --percentage 5   # ~4,500 images\n",
    "python scripts/subset_data.py --percentage 10  # ~9,000 images\n",
    "python scripts/subset_data.py --percentage 25  # ~22,500 images\n",
    "python scripts/subset_data.py --percentage 50  # ~45,000 images\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ba276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Scaling Recommendations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "For production deployment on AWS EMR or Databricks:\n",
    "\n",
    "1. STORAGE:\n",
    "   - Upload images to S3 bucket (EU region for RGPD)\n",
    "   - Use s3a:// protocol for Spark access\n",
    "   - Store results in Parquet format\n",
    "\n",
    "2. COMPUTE:\n",
    "   - EMR cluster with GPU instances (p3.2xlarge)\n",
    "   - Or Databricks with GPU runtime\n",
    "   - Scale executors based on dataset size\n",
    "\n",
    "3. OPTIMIZATION:\n",
    "   - Increase spark.sql.execution.arrow.maxRecordsPerBatch\n",
    "   - Use spark.sql.shuffle.partitions = 2x num_cores\n",
    "   - Enable adaptive query execution\n",
    "\n",
    "4. RGPD COMPLIANCE:\n",
    "   - Use EU-west-1 or EU-central-1 regions\n",
    "   - Enable S3 bucket encryption\n",
    "   - Configure VPC for network isolation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56820fab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrates a complete Big Data image processing pipeline:\n",
    "\n",
    "1. **Data Loading**: PySpark binary file reader for distributed image loading\n",
    "2. **Feature Extraction**: GPU-accelerated MobileNetV2 transfer learning\n",
    "3. **Dimensionality Reduction**: PCA from 1280 to 50 components\n",
    "4. **Results Export**: Parquet and CSV formats for cloud storage\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Images | ~90,000 |\n",
    "| Original Features | 1,280 |\n",
    "| PCA Components | 50 |\n",
    "| Explained Variance | ~95% |\n",
    "| Compression Ratio | 25.6x |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Deploy to AWS EMR or Databricks for cloud execution\n",
    "2. Train classification model on PCA features\n",
    "3. Implement real-time inference API\n",
    "4. Set up continuous training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"\\n✓ Spark session stopped\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pipeline execution complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
