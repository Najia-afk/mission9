{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be387c58",
   "metadata": {},
   "source": [
    "# Mission 9: Big Data Processing on AWS EMR\n",
    "\n",
    "## PySpark Processing Pipeline - PRODUCTION\n",
    "\n",
    "This notebook runs the complete pipeline:\n",
    "1. **Loading**: Images from S3 (full dataset ~90K images)\n",
    "2. **Feature extraction**: MobileNetV2 with broadcast weights\n",
    "3. **PCA**: Dimensionality reduction (1280 â†’ 50)\n",
    "4. **Export**: CSV to S3\n",
    "\n",
    "### GDPR Compliance\n",
    "- Region: **eu-west-1** (Ireland) - European servers\n",
    "- Data stored and processed in the EU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13861fde",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Spark Configuration for EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0736ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set YARN as master for EMR cluster\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master yarn pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with optimized settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mission9_Fruits_PROD\") \\\n",
    "    .config(\"spark.sql.parquet.writeLegacyFormat\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Application ID: {sc.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f24d70",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.1 Dependencies Fix (urllib3/OpenSSL)\n",
    "\n",
    "> **Important**: EMR uses Python 3.7 with OpenSSL 1.0.2k. \n",
    "> urllib3 v2.0 requires OpenSSL 1.1.1+. This cell fixes compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix urllib3/OpenSSL compatibility + install boto3\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Install to /tmp (writable) and add to path FIRST\n",
    "install_dir = '/tmp/pip_packages'\n",
    "os.makedirs(install_dir, exist_ok=True)\n",
    "\n",
    "# Add to path BEFORE any imports\n",
    "if install_dir not in sys.path:\n",
    "    sys.path.insert(0, install_dir)\n",
    "\n",
    "# Remove old urllib3 from cache if loaded\n",
    "mods_to_remove = [m for m in sys.modules if m.startswith('urllib3') or m.startswith('boto')]\n",
    "for m in mods_to_remove:\n",
    "    del sys.modules[m]\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, '-m', 'pip', 'install', '--target', install_dir, '--upgrade', '--force-reinstall',\n",
    "     'urllib3<2.0', 'boto3'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print(\"Install completed\" if result.returncode == 0 else f\"Error: {result.stderr[-300:]}\")\n",
    "\n",
    "# Verify\n",
    "import urllib3\n",
    "import boto3\n",
    "print(f\"urllib3: {urllib3.__version__} from {urllib3.__file__}\")\n",
    "print(f\"boto3: {boto3.__version__} from {boto3.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09fa7e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. S3 Configuration - Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2367ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Auto-discover S3 bucket name\n",
    "BUCKET_NAME = [b['Name'] for b in boto3.client('s3').list_buckets()['Buckets'] if 'mission9-data' in b['Name']][0]\n",
    "\n",
    "# Define S3 paths (Training only - ~67K images)\n",
    "BUCKET = f's3://{BUCKET_NAME}'\n",
    "PATH_Data = BUCKET + '/fruits-360_dataset/fruits-360/Training'\n",
    "PATH_Result = BUCKET + '/Results'\n",
    "PATH_PCA = BUCKET + '/Results_PCA'\n",
    "PATH_CSV = BUCKET + '/Results_CSV'\n",
    "\n",
    "print(f'BUCKET: {BUCKET}')\n",
    "print(f'Dataset: Training only')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a66b09",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# TensorFlow / MobileNetV2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# PySpark ML\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973cce3b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Images from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading images from: {PATH_Data}\")\n",
    "\n",
    "# Load all JPG images recursively from S3\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(PATH_Data)\n",
    "\n",
    "# Extract label from folder name\n",
    "images = images.withColumn('label', element_at(split(images['path'], '/'), -2))\n",
    "images.cache()\n",
    "\n",
    "total_images = images.count()\n",
    "labels = images.select('label').distinct().count()\n",
    "\n",
    "print(f\"Images loaded: {total_images:,}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Partitions: {images.rdd.getNumPartitions()}\")\n",
    "images.select('path', 'label').show(5, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d15b2",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. MobileNetV2 Model + Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MobileNetV2...\")\n",
    "\n",
    "# Load pre-trained model (ImageNet weights)\n",
    "model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "\n",
    "# Remove last layer to get 1280-dim features\n",
    "new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "# Freeze weights (inference only)\n",
    "for layer in new_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Broadcast weights to all workers\n",
    "broadcast_weights = sc.broadcast(new_model.get_weights())\n",
    "\n",
    "print(f\"Model loaded - Output: {new_model.output_shape}\")\n",
    "print(f\"Weights broadcasted ({len(new_model.get_weights())} arrays)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a02e30",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Featurization Functions (Pandas UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model on workers with broadcasted weights\n",
    "def model_fn():\n",
    "    model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    return new_model\n",
    "\n",
    "# Preprocess image for MobileNetV2\n",
    "def preprocess(content):\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "# Extract features for a batch\n",
    "def featurize_series(model, content_series):\n",
    "    input_data = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input_data, verbose=0)\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "# Pandas UDF for distributed processing\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)\n",
    "\n",
    "print(\"Featurization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e595d2",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Extraction (Distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Optimize batch size for Arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")\n",
    "NUM_PARTITIONS = 48\n",
    "\n",
    "print(f\"Extracting features... (partitions: {NUM_PARTITIONS})\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Repartition and apply featurization UDF\n",
    "features_df = images.repartition(NUM_PARTITIONS).select(\n",
    "    col(\"path\"),\n",
    "    col(\"label\"),\n",
    "    featurize_udf(\"content\").alias(\"features\")\n",
    ")\n",
    "\n",
    "# Save to Parquet on S3\n",
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Done in {elapsed/60:.1f} min -> {PATH_Result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58772a",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Dimensionality Reduction with PCA (Spark ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload features from Parquet\n",
    "features_df = spark.read.parquet(PATH_Result)\n",
    "print(f\"Features loaded: {features_df.count()} images\")\n",
    "\n",
    "# Convert array to Spark ML Vector\n",
    "@udf(VectorUDT())\n",
    "def array_to_vector(arr):\n",
    "    return Vectors.dense(arr)\n",
    "\n",
    "features_vec_df = features_df.withColumn(\"features_vec\", array_to_vector(col(\"features\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f3818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (mean=0, std=1)\n",
    "print(\"Standardization...\")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_vec\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "scaler_model = scaler.fit(features_vec_df)\n",
    "features_scaled_df = scaler_model.transform(features_vec_df)\n",
    "print(\"Standardization done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d133ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: reduce 1280 dimensions to 50\n",
    "N_COMPONENTS = 50\n",
    "\n",
    "print(f\"PCA in progress ({N_COMPONENTS} components)...\")\n",
    "pca = PCA(\n",
    "    k=N_COMPONENTS,\n",
    "    inputCol=\"features_scaled\",\n",
    "    outputCol=\"pca_features\"\n",
    ")\n",
    "pca_model = pca.fit(features_scaled_df)\n",
    "features_pca_df = pca_model.transform(features_scaled_df)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance = pca_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "print(\"PCA done\")\n",
    "print(f\"Total explained variance: {cumulative_variance[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66ff4d",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export Results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Vector back to array for export\n",
    "@udf(ArrayType(FloatType()))\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().tolist()\n",
    "\n",
    "# Select final columns\n",
    "output_df = features_pca_df.select(\n",
    "    \"path\",\n",
    "    \"label\",\n",
    "    vector_to_array(\"pca_features\").alias(\"pca_features\")\n",
    ")\n",
    "\n",
    "# Save as Parquet\n",
    "output_df.write.mode(\"overwrite\").parquet(PATH_PCA)\n",
    "print(f\"Parquet saved: {PATH_PCA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c44c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV with individual columns (f_0, f_1, ..., f_49)\n",
    "print(\"CSV export...\")\n",
    "csv_df = output_df.select(\"label\", \"pca_features\")\n",
    "\n",
    "# Create individual feature columns\n",
    "for i in range(N_COMPONENTS):\n",
    "    csv_df = csv_df.withColumn(f\"f_{i}\", col(\"pca_features\")[i])\n",
    "\n",
    "feature_cols = [f\"f_{i}\" for i in range(N_COMPONENTS)]\n",
    "csv_df = csv_df.select(\"label\", *feature_cols)\n",
    "\n",
    "# Save as single CSV file\n",
    "csv_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(PATH_CSV)\n",
    "print(f\"CSV saved: {PATH_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba09e7",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Validation and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db66165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation\n",
    "final_count = output_df.count()\n",
    "sample = output_df.limit(5).toPandas()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY - MISSION 9\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Images processed:      {final_count:,}\")\n",
    "print(f\"Classes (labels):      {labels}\")\n",
    "print(f\"Original dimension:    1280 (MobileNetV2)\")\n",
    "print(f\"PCA dimension:         {N_COMPONENTS}\")\n",
    "print(f\"Explained variance:    {cumulative_variance[-1]*100:.2f}%\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"S3 Bucket:             {BUCKET}\")\n",
    "print(f\"Parquet:               {PATH_PCA}\")\n",
    "print(f\"CSV:                   {PATH_CSV}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSample data (5 rows):\")\n",
    "display(sample) if 'display' in dir() else print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c16af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean shutdown\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
