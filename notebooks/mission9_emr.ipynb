{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be387c58",
   "metadata": {},
   "source": [
    "# Mission 9: Big Data Processing on AWS EMR\n",
    "\n",
    "## PySpark Processing Pipeline - PRODUCTION\n",
    "\n",
    "This notebook runs the complete pipeline:\n",
    "1. **Loading**: Images from S3 (full dataset ~90K images)\n",
    "2. **Feature extraction**: MobileNetV2 with broadcast weights\n",
    "3. **PCA**: Dimensionality reduction (1280 → 50)\n",
    "4. **Export**: CSV to S3\n",
    "\n",
    "### GDPR Compliance\n",
    "- Region: **eu-west-1** (Ireland) - European servers\n",
    "- Data stored and processed in the EU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13861fde",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Spark Configuration for EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0736ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1768169693295_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-45-10.eu-west-1.compute.internal:20888/proxy/application_1768169693295_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-38-218.eu-west-1.compute.internal:8042/node/containerlogs/container_1768169693295_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.4.1-amzn-2\n",
      "Master: yarn\n",
      "Application ID: application_1768169693295_0001"
     ]
    }
   ],
   "source": [
    "# Set YARN as master for EMR cluster\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master yarn pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with optimized settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mission9_Fruits_PROD\") \\\n",
    "    .config(\"spark.sql.parquet.writeLegacyFormat\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Application ID: {sc.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f24d70",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.1 Dependencies Fix (urllib3/OpenSSL)\n",
    "\n",
    "> **Important**: EMR uses Python 3.7 with OpenSSL 1.0.2k. \n",
    "> urllib3 v2.0 requires OpenSSL 1.1.1+. This cell fixes compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce3edd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install completed\n",
      "urllib3: 1.26.20 from /tmp/pip_packages/urllib3/__init__.py\n",
      "boto3: 1.33.13 from /tmp/pip_packages/boto3/__init__.py"
     ]
    }
   ],
   "source": [
    "# Fix urllib3/OpenSSL compatibility + install boto3\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Install to /tmp (writable) and add to path FIRST\n",
    "install_dir = '/tmp/pip_packages'\n",
    "os.makedirs(install_dir, exist_ok=True)\n",
    "\n",
    "# Add to path BEFORE any imports\n",
    "if install_dir not in sys.path:\n",
    "    sys.path.insert(0, install_dir)\n",
    "\n",
    "# Remove old urllib3 from cache if loaded\n",
    "mods_to_remove = [m for m in sys.modules if m.startswith('urllib3') or m.startswith('boto')]\n",
    "for m in mods_to_remove:\n",
    "    del sys.modules[m]\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, '-m', 'pip', 'install', '--target', install_dir, '--upgrade', '--force-reinstall',\n",
    "     'urllib3<2.0', 'boto3'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print(\"Install completed\" if result.returncode == 0 else f\"Error: {result.stderr[-300:]}\")\n",
    "\n",
    "# Verify\n",
    "import urllib3\n",
    "import boto3\n",
    "print(f\"urllib3: {urllib3.__version__} from {urllib3.__file__}\")\n",
    "print(f\"boto3: {boto3.__version__} from {boto3.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09fa7e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. S3 Configuration - Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2367ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET: s3://mission9-data-20260111202728878200000001\n",
      "Dataset: Training only\n",
      "/tmp/pip_packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Auto-discover S3 bucket name\n",
    "BUCKET_NAME = [b['Name'] for b in boto3.client('s3').list_buckets()['Buckets'] if 'mission9-data' in b['Name']][0]\n",
    "\n",
    "# Define S3 paths (Training only - ~67K images)\n",
    "BUCKET = f's3://{BUCKET_NAME}'\n",
    "PATH_Data = BUCKET + '/fruits-360_dataset/fruits-360/Training'\n",
    "PATH_Result = BUCKET + '/Results'\n",
    "PATH_PCA = BUCKET + '/Results_PCA'\n",
    "PATH_CSV = BUCKET + '/Results_CSV'\n",
    "\n",
    "print(f'BUCKET: {BUCKET}')\n",
    "print(f'Dataset: Training only')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a66b09",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd0414f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.11.0"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# TensorFlow / MobileNetV2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# PySpark ML\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973cce3b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Images from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122c1d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: s3://mission9-data-20260111202728878200000001/fruits-360_dataset/fruits-360/Training\n",
      "Images loaded: 67,692\n",
      "Labels: 131\n",
      "Partitions: 2116\n",
      "+------------------------------------------------------------+--------------+\n",
      "|                                                        path|         label|\n",
      "+------------------------------------------------------------+--------------+\n",
      "|s3://mission9-data-20260111202728878200000001/fruits-360_...|     Raspberry|\n",
      "|s3://mission9-data-20260111202728878200000001/fruits-360_...|     Raspberry|\n",
      "|s3://mission9-data-20260111202728878200000001/fruits-360_...|Pineapple Mini|\n",
      "|s3://mission9-data-20260111202728878200000001/fruits-360_...|     Raspberry|\n",
      "|s3://mission9-data-20260111202728878200000001/fruits-360_...|     Raspberry|\n",
      "+------------------------------------------------------------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "print(f\"Loading images from: {PATH_Data}\")\n",
    "\n",
    "# Load all JPG images recursively from S3\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(PATH_Data)\n",
    "\n",
    "# Extract label from folder name\n",
    "images = images.withColumn('label', element_at(split(images['path'], '/'), -2))\n",
    "images.cache()\n",
    "\n",
    "total_images = images.count()\n",
    "labels = images.select('label').distinct().count()\n",
    "\n",
    "print(f\"Images loaded: {total_images:,}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Partitions: {images.rdd.getNumPartitions()}\")\n",
    "images.select('path', 'label').show(5, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d15b2",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. MobileNetV2 Model + Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2568de04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MobileNetV2...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "14536120/14536120 [==============================] - 1s 0us/step\n",
      "Model loaded - Output: (None, 1280)\n",
      "Weights broadcasted (260 arrays)"
     ]
    }
   ],
   "source": [
    "print(\"Loading MobileNetV2...\")\n",
    "\n",
    "# Load pre-trained model (ImageNet weights)\n",
    "model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "\n",
    "# Remove last layer to get 1280-dim features\n",
    "new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "# Freeze weights (inference only)\n",
    "for layer in new_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Broadcast weights to all workers\n",
    "broadcast_weights = sc.broadcast(new_model.get_weights())\n",
    "\n",
    "print(f\"Model loaded - Output: {new_model.output_shape}\")\n",
    "print(f\"Weights broadcasted ({len(new_model.get_weights())} arrays)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a02e30",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Featurization Functions (Pandas UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7772f83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurization functions defined\n",
      "/mnt/yarn/usercache/livy/appcache/application_1768169693295_0001/container_1768169693295_0001_01_000001/pyspark.zip/pyspark/sql/pandas/functions.py:403: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details."
     ]
    }
   ],
   "source": [
    "# Recreate model on workers with broadcasted weights\n",
    "def model_fn():\n",
    "    model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    return new_model\n",
    "\n",
    "# Preprocess image for MobileNetV2\n",
    "def preprocess(content):\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "# Extract features for a batch\n",
    "def featurize_series(model, content_series):\n",
    "    input_data = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input_data, verbose=0)\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "# Pandas UDF for distributed processing\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)\n",
    "\n",
    "print(\"Featurization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e595d2",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Extraction (Distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefe1cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features... (partitions: 48)\n",
      "Done in 21.5 min -> s3://mission9-data-20260111202728878200000001/Results"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Optimize batch size for Arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")\n",
    "NUM_PARTITIONS = 48\n",
    "\n",
    "print(f\"Extracting features... (partitions: {NUM_PARTITIONS})\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Repartition and apply featurization UDF\n",
    "features_df = images.repartition(NUM_PARTITIONS).select(\n",
    "    col(\"path\"),\n",
    "    col(\"label\"),\n",
    "    featurize_udf(\"content\").alias(\"features\")\n",
    ")\n",
    "\n",
    "# Save to Parquet on S3\n",
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Done in {elapsed/60:.1f} min -> {PATH_Result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58772a",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Dimensionality Reduction with PCA (Spark ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b11bfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded: 67692 images"
     ]
    }
   ],
   "source": [
    "# Reload features from Parquet\n",
    "features_df = spark.read.parquet(PATH_Result)\n",
    "print(f\"Features loaded: {features_df.count()} images\")\n",
    "\n",
    "# Convert array to Spark ML Vector\n",
    "@udf(VectorUDT())\n",
    "def array_to_vector(arr):\n",
    "    return Vectors.dense(arr)\n",
    "\n",
    "features_vec_df = features_df.withColumn(\"features_vec\", array_to_vector(col(\"features\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a77f3818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization...\n",
      "Standardization done"
     ]
    }
   ],
   "source": [
    "# Standardize features (mean=0, std=1)\n",
    "print(\"Standardization...\")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_vec\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "scaler_model = scaler.fit(features_vec_df)\n",
    "features_scaled_df = scaler_model.transform(features_vec_df)\n",
    "print(\"Standardization done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74d133ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA in progress (50 components)...\n",
      "PCA done\n",
      "Total explained variance: 58.39%"
     ]
    }
   ],
   "source": [
    "# PCA: reduce 1280 dimensions to 50\n",
    "N_COMPONENTS = 50\n",
    "\n",
    "print(f\"PCA in progress ({N_COMPONENTS} components)...\")\n",
    "pca = PCA(\n",
    "    k=N_COMPONENTS,\n",
    "    inputCol=\"features_scaled\",\n",
    "    outputCol=\"pca_features\"\n",
    ")\n",
    "pca_model = pca.fit(features_scaled_df)\n",
    "features_pca_df = pca_model.transform(features_scaled_df)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance = pca_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "print(\"PCA done\")\n",
    "print(f\"Total explained variance: {cumulative_variance[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66ff4d",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export Results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a54a883d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet saved: s3://mission9-data-20260111202728878200000001/Results_PCA"
     ]
    }
   ],
   "source": [
    "# Convert Vector back to array for export\n",
    "@udf(ArrayType(FloatType()))\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().tolist()\n",
    "\n",
    "# Select final columns\n",
    "output_df = features_pca_df.select(\n",
    "    \"path\",\n",
    "    \"label\",\n",
    "    vector_to_array(\"pca_features\").alias(\"pca_features\")\n",
    ")\n",
    "\n",
    "# Save as Parquet\n",
    "output_df.write.mode(\"overwrite\").parquet(PATH_PCA)\n",
    "print(f\"Parquet saved: {PATH_PCA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c44c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV export...\n",
      "CSV saved: s3://mission9-data-20260111202728878200000001/Results_CSV"
     ]
    }
   ],
   "source": [
    "# Export to CSV with individual columns (f_0, f_1, ..., f_49)\n",
    "print(\"CSV export...\")\n",
    "csv_df = output_df.select(\"label\", \"pca_features\")\n",
    "\n",
    "# Create individual feature columns\n",
    "for i in range(N_COMPONENTS):\n",
    "    csv_df = csv_df.withColumn(f\"f_{i}\", col(\"pca_features\")[i])\n",
    "\n",
    "feature_cols = [f\"f_{i}\" for i in range(N_COMPONENTS)]\n",
    "csv_df = csv_df.select(\"label\", *feature_cols)\n",
    "\n",
    "# Save as single CSV file\n",
    "csv_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(PATH_CSV)\n",
    "print(f\"CSV saved: {PATH_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba09e7",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Validation and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db66165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUMMARY - MISSION 9\n",
      "============================================================\n",
      "Images processed:      67,692\n",
      "Classes (labels):      131\n",
      "Original dimension:    1280 (MobileNetV2)\n",
      "PCA dimension:         50\n",
      "Explained variance:    58.39%\n",
      "------------------------------------------------------------\n",
      "S3 Bucket:             s3://mission9-data-20260111202728878200000001\n",
      "Parquet:               s3://mission9-data-20260111202728878200000001/Results_PCA\n",
      "CSV:                   s3://mission9-data-20260111202728878200000001/Results_CSV\n",
      "============================================================\n",
      "\n",
      "Sample data (5 rows):\n",
      "                                                path  ...                                       pca_features\n",
      "0  s3://mission9-data-20260111202728878200000001/...  ...  [-9.348203, 5.983967, 3.1709373, 13.749005, 11...\n",
      "1  s3://mission9-data-20260111202728878200000001/...  ...  [-3.417855, -0.3172249, 1.2731464, 19.171167, ...\n",
      "2  s3://mission9-data-20260111202728878200000001/...  ...  [-10.747551, 7.1448207, 2.0115633, 14.50745, 1...\n",
      "3  s3://mission9-data-20260111202728878200000001/...  ...  [-8.889944, 5.487995, 3.6472237, 16.027466, 11...\n",
      "4  s3://mission9-data-20260111202728878200000001/...  ...  [-10.574188, 5.114107, 2.9546745, 16.098436, 1...\n",
      "\n",
      "[5 rows x 3 columns]"
     ]
    }
   ],
   "source": [
    "# Final validation\n",
    "final_count = output_df.count()\n",
    "sample = output_df.limit(5).toPandas()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY - MISSION 9\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Images processed:      {final_count:,}\")\n",
    "print(f\"Classes (labels):      {labels}\")\n",
    "print(f\"Original dimension:    1280 (MobileNetV2)\")\n",
    "print(f\"PCA dimension:         {N_COMPONENTS}\")\n",
    "print(f\"Explained variance:    {cumulative_variance[-1]*100:.2f}%\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"S3 Bucket:             {BUCKET}\")\n",
    "print(f\"Parquet:               {PATH_PCA}\")\n",
    "print(f\"CSV:                   {PATH_CSV}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSample data (5 rows):\")\n",
    "display(sample) if 'display' in dir() else print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c16af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean shutdown\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
